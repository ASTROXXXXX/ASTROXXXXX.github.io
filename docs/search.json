[
  {
    "objectID": "Childcare Costs.html",
    "href": "Childcare Costs.html",
    "title": "Childcare Costs",
    "section": "",
    "text": "This dataset comes from the National Database of Childcare Prices (NDCP), one of the most comprehensive federal sources of childcare data at the county level. It includes detailed cost information by age group, geography, and provider type. The data was prepared for the TidyTuesday project by Thomas Mock and is available publicly for analysis.\nlibrary(tidyverse)\n\nchildcare_costs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-05-09/childcare_costs.csv')\n\nchildcare_costs |&gt;\nggplot( aes( x = mcsa, y = pr_f ) ) +\n  geom_point(color = \"blue\", alpha = 0.1) +\n  labs( x = \"median price charged for center-based care who are school age\",\n        y = \"poverty rate for familie\" )"
  },
  {
    "objectID": "Strings.html",
    "href": "Strings.html",
    "title": "Macbeth",
    "section": "",
    "text": "This dataset contains the full script of the famous Shakespearean play Macbeth. As a side note, I was really into acting back in middle school and actually played a character (whose name I’ve forgotten—it wasn’t a major role) in Macbeth during one of our school productions. I’m excited to revisit the play from a data perspective—let’s dive in!"
  },
  {
    "objectID": "Ethics.html",
    "href": "Ethics.html",
    "title": "Ethics in Data Science",
    "section": "",
    "text": "The COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) algorithm was developed by a for-profit company called Northpointe. It is used in courtrooms across the U.S. to help inform sentencing decisions, set bail/bond amounts, and assist with other decision-making in the criminal justice system (Angwin et al., 2016). The algorithm’s key feature is to produce “risk ratings” for defendants based on their criminal history and their responses to a questionnaire of more than 100 questions (Angwin et al., 2016). A major ethical concern is that the algorithm appears to be racially and gender sensitive. It was found to be “particularly likely to falsely flag Black defendants as future criminals, wrongly labeling them this way at almost twice the rate of white defendants” (Angwin et al., 2016), and that “women were rated two classes worse than men” (Drosser et al., 2020).\nIt is easy to conclude that the algorithm is biased—or even racist or sexist—against defendants, but the issue may be deeper than that. For one thing, the algorithm seems accurate in predicting recidivism rates across races. In other words, there is a similar proportion of white defendants who will re-offend compared to Black defendants with the same risk rating (Corbett-Davies et al., 2018), and that appears to be what the risk ratings were initially designed to reflect. It has been proposed that maybe it is impossible for an algorithm to achieve both:\n\nWell-calibrated risk between groups (where risk ratings mean the same thing across groups), and\nEqual false positive rates (not falsely flagging certain groups more than others).\n\nMaybe this is achievable in a perfect society where everyone receives equal resources, education, etc. But in our current world—where inequality and systemic discrimination are common, especially due to historical factors—racial groups may unfortunately correlate with the likelihood to commit (or recommit) a crime. This is known as base rate differences. That correlation may be the reason why it’s impossible to achieve both fairness criteria at once. (This argument is mainly derived from Corbett-Davies et al., 2018.)"
  },
  {
    "objectID": "Simulation Analysis.html",
    "href": "Simulation Analysis.html",
    "title": "Simulation!",
    "section": "",
    "text": "Assuming that there are four course registration periods over four years of a student’s time at Pomona College (there are actually 8), and course registration happens once per year. Suppose that (drawing from personal experience) the college’s algorithm is designed so that it categorizes registration times into:\n\n1. Early: 8 ~ 10am\n2. Moderately Early: 10 ~ noon\n3. Moderately Late: noon ~ 2pm\n4. Late: 2 ~ 4pm\n\nand suppose that during course registration the algorithm first randomly determines a category and then randomly assigns a time (by each minute) within that category to each student and also makes sure that each category is assigned to each student only once over their four years at the college (i.e four course registrations).\nSuppose that the best thing to happen is either A. a freshman gets a time before 9am (so that he/she can beat the other freshmen in the same category) B. a sophomore gets a time before 11am. I want to use simulation to find the probability of A., B. and for the luckiest ones, A. and B. both happen.\n\nlibrary(tidyverse)\nset.seed(47)\n\n# Simulation function for course registration scenario\ncorsreg_func &lt;- function(banana){\n  cats = sample(c(1:4), 4, replace = FALSE)  # randomize category order\n  times = sample(c(1:120), 4, replace = TRUE)  # assign time in each category\n\n  data.frame(\n    lucky_freshman = (cats[1] == 1 && times[1] &lt;= 59),\n    lucky_sophomore = (cats[2] == 1 | cats[2] == 2 && times[2] &lt;= 59),\n    luckiest_one = ((cats[1] == 1 && times[1] &lt;= 59) && (cats[2] == 1 | cats[2] == 2 && times[2] &lt;= 59))\n  )\n}\n\nn &lt;- 1761  # approximate number of Pomona students\ncorsreg_data &lt;- map(1:n, corsreg_func) |&gt; list_rbind()\n\nsummary_data &lt;- corsreg_data |&gt; \n  summarise(\n    lucky_freshman_prop = mean(lucky_freshman),\n    lucky_sophomore_prop = mean(lucky_sophomore),\n    luckiest_one_prop = mean(luckiest_one)\n  ) |&gt; \n  pivot_longer(cols = everything(), names_to = \"category\", values_to = \"proportion\")\n\nsummary_data$category &lt;- factor(summary_data$category, \n                                 levels = c(\"lucky_freshman_prop\", \"lucky_sophomore_prop\", \"luckiest_one_prop\"))\n\n# Visualization\nggplot(summary_data, aes(x = category, y = proportion, fill = category)) +\n  geom_bar(stat = \"identity\", width = 0.6) +\n  labs(\n    title = \"Course Registration Simulation at Pomona College\",\n    x = \"I simulated 1761 times to approximate the number of students on campus\",\n    y = \"Proportion\",\n    alt = \"Bar graph showing approximate proportions of students with early course registration times in freshman (11%), sophomore (36%), and both years (2%)\"\n  ) +\n  theme_minimal() +\n  geom_text(aes(label = round(proportion, 3)), vjust = -0.5) +  \n  ylim(0, 1) +\n  theme(axis.text.x = element_blank())\n\n\n\n\n\n\n\n\nFigure 1: Proportion of students who were lucky in freshman, sophomore, or both years of registration.\nSeems like there is around 11% probability for a student to be lucky in their course registrations during their freshman year, and a much greater probability (around 36%) for him/her to be lucky in their sophomore year. But getting extremely lucky, where he/she get an early time for both years is extremely rare (around 2%). Despite the fact that I believe getting a time before 9am is pretty important for a freshman to get into a class they wanted, knowing that there is almost 40% students who will get an ideal time in their sophomore year is pretty relieving. Some lucky sophomores may also be lucky in their freshman year, but that rate is low (around 2%). I personally think that the algorithm, as presented here, seems to be decent, because we are getting ideal results for almost 40% of the students in their sophomore year (which is pretty high). But things could also be said about the pretty low rate (11%) to be lucky freshman. But it is also important to note that we have other mechanisms to distribute courses to students who actually need them (or need them more), such as the PERM system or considerations on individual basis.\n\n\nProbability of Being Lucky Freshman OR Sophomore\n# Add OR case to the dataset and compute the proportion\ncorsreg_data &lt;- corsreg_data |&gt; \n  mutate(lucky_either = lucky_freshman | lucky_sophomore)\n\nor_summary &lt;- summarise(corsreg_data, prob_lucky_either = mean(lucky_either))\nor_summary\n\n\n  prob_lucky_either\n1         0.4497445\n\n\nI also calculated the probability that a student is lucky in either their freshman or sophomore year. This came out to be approximately 45%, which is substantially higher than the probability of being lucky in both years (only ~2%). This suggests that although it’s rare for someone to get great registration times in both years, it’s quite likely they’ll have at least one year with a favorable time. This adds further evidence that the registration system provides a fair spread of opportunity over time."
  },
  {
    "objectID": "SQL.html",
    "href": "SQL.html",
    "title": "Stanford Open Policing Project",
    "section": "",
    "text": "# Connect to the Stanford Open Policing Project database using environment variables\nlibrary(DBI)\ncon_traffic &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"traffic\",\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n)\nThis project uses SQL to analyze the Stanford Open Policing Project (SOPP) dataset, which contains traffic stop records from multiple U.S. states over several years. The goal is to explore the following questions:"
  },
  {
    "objectID": "The Scent of Data.html",
    "href": "The Scent of Data.html",
    "title": "The Scent of Data",
    "section": "",
    "text": "This dataset contains information about perfumes webscraped from Parfumo, a community of perfume enthusiasts. The data was originally collected and made publicly available by Olga G., and was later featured in the TidyTuesday project for public data exploration and visualization.\nPlay around with the interactive dashboard below to explore perfume rating distributions by brand presented by box plots. The goal is to visually analyze trends and preferences across different perfume manufacturers based on community ratings.\nData sourced from:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zhesheng(Jason) Xu",
    "section": "",
    "text": "Hi there! I’m Jason, my hometown is Shanghai, China! I am currently a student at Pomona College, CA, where I am studying neuroscience and data science. I am also a part of a club called Health Bridges where I get to volunteer at Pomona Valley Hospital as a non-medical translator! In my free time, I like to play the guitar (Chinese folk songs are my favorite!). Look around my website to learn more!\nI used Quarto to build this website, the link to the repository is here: https://github.com/ASTROXXXXX/ASTROXXXXX.github.io"
  },
  {
    "objectID": "DS002 Final Prez.html#welcometo-the-world-of-macbeth",
    "href": "DS002 Final Prez.html#welcometo-the-world-of-macbeth",
    "title": "Behind the Scenes",
    "section": "Welcome…to the world of Macbeth",
    "text": "Welcome…to the world of Macbeth\n\n\n\n\n\nMe in 6th grade:"
  },
  {
    "objectID": "DS002 Final Prez.html#the-data",
    "href": "DS002 Final Prez.html#the-data",
    "title": "Behind the Scenes",
    "section": "The Data",
    "text": "The Data\n“macbeth” from TidyTuesday!\n\nlibrary(dplyr)\nlibrary(tidyverse)\n\nmacbeth &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-17/macbeth.csv')\n\nhead(macbeth)\n\n# A tibble: 6 × 5\n  act   scene   character         dialogue                           line_number\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;             &lt;chr&gt;                                    &lt;dbl&gt;\n1 Act I Scene I [stage direction] Thunder and lightning. Enter thre…          NA\n2 Act I Scene I First Witch       When shall we three meet again               1\n3 Act I Scene I First Witch       In thunder, lightning, or in rain?           2\n4 Act I Scene I Second Witch      When the hurlyburly's done,                  3\n5 Act I Scene I Second Witch      When the battle's lost and won.              4\n6 Act I Scene I Third Witch       That will be ere the set of sun.             5"
  },
  {
    "objectID": "DS002 Final Prez.html#negative-words",
    "href": "DS002 Final Prez.html#negative-words",
    "title": "Behind the Scenes",
    "section": "Negative Words",
    "text": "Negative Words\nWhich lines (or dialogue) contain negative words?\nI’ll call it Negative Lines\n\nneg_lines &lt;- macbeth |&gt;\n  mutate( blood = str_detect( dialogue, \"(?i)blood|(?i)murder|(?i)death|(?i)kill\" ) ) |&gt;\n  select( dialogue, blood)\n\nhead(neg_lines)\n\n# A tibble: 6 × 2\n  dialogue                                   blood\n  &lt;chr&gt;                                      &lt;lgl&gt;\n1 Thunder and lightning. Enter three Witches FALSE\n2 When shall we three meet again             FALSE\n3 In thunder, lightning, or in rain?         FALSE\n4 When the hurlyburly's done,                FALSE\n5 When the battle's lost and won.            FALSE\n6 That will be ere the set of sun.           FALSE"
  },
  {
    "objectID": "DS002 Final Prez.html#negative-lines-per-act",
    "href": "DS002 Final Prez.html#negative-lines-per-act",
    "title": "Behind the Scenes",
    "section": "Negative Lines Per Act",
    "text": "Negative Lines Per Act\nTotal number of negative lines per act\n\nneg_lines &lt;- macbeth |&gt;\n  mutate( blood = str_detect( dialogue, \"(?i)blood|(?i)murder|(?i)death|(?i)kill\" ) ) |&gt;\n  filter( blood ) |&gt;\n  group_by( act ) |&gt;\n  summarize( negative_lines_per_act = n() ) \n\nneg_lines\n\n# A tibble: 5 × 2\n  act     negative_lines_per_act\n  &lt;chr&gt;                    &lt;int&gt;\n1 Act I                       15\n2 Act II                      31\n3 Act III                     27\n4 Act IV                      14\n5 Act V                       10"
  },
  {
    "objectID": "DS002 Final Prez.html#string-length",
    "href": "DS002 Final Prez.html#string-length",
    "title": "Behind the Scenes",
    "section": "String Length",
    "text": "String Length\nString length counts the number of letters/characters/spaces in a string.\nIt is one way to quantify the length of each sentence of the dialogue.\n\nstring_length = str_length( macbeth$dialogue )\n\nhead(string_length)                        \n\n[1] 42 30 34 27 31 32"
  },
  {
    "objectID": "DS002 Final Prez.html#average-string-length-per-act",
    "href": "DS002 Final Prez.html#average-string-length-per-act",
    "title": "Behind the Scenes",
    "section": "Average String Length per Act",
    "text": "Average String Length per Act\n\naverage_character &lt;- macbeth |&gt;\n  mutate( string_length = str_length( dialogue ) ) |&gt;\n  group_by( act ) |&gt;\n  summarize( average_character = mean( string_length ) ) \n\naverage_character\n\n# A tibble: 5 × 2\n  act     average_character\n  &lt;chr&gt;               &lt;dbl&gt;\n1 Act I                36.3\n2 Act II               33.9\n3 Act III              36.1\n4 Act IV               34.7\n5 Act V                36.4"
  },
  {
    "objectID": "DS002 Final Prez.html#is-there-a-relationship",
    "href": "DS002 Final Prez.html#is-there-a-relationship",
    "title": "Behind the Scenes",
    "section": "Is there a relationship?",
    "text": "Is there a relationship?\nHypothesis: Characters in acts that contain more negative lines also tend to speak longer sentences (i.e longer string length).\n\nnegative_words &lt;- left_join( neg_lines, average_character )\n\nhead(negative_words)\n\n# A tibble: 5 × 3\n  act     negative_lines_per_act average_character\n  &lt;chr&gt;                    &lt;int&gt;             &lt;dbl&gt;\n1 Act I                       15              36.3\n2 Act II                      31              33.9\n3 Act III                     27              36.1\n4 Act IV                      14              34.7\n5 Act V                       10              36.4"
  },
  {
    "objectID": "DS002 Final Prez.html#is-there-a-relationship-1",
    "href": "DS002 Final Prez.html#is-there-a-relationship-1",
    "title": "Behind the Scenes",
    "section": "Is there a relationship?",
    "text": "Is there a relationship?\n\n\n\n\nOpenning and closing (I and V) acts have the longest dialogues, but low negative lines\nIf any relationship: total negative lines per act seems to be negatively associated to average dialogue length per act -&gt; but comfounding variable: some acts may be longer than others\nAct II has the most negative lines, but the shortest dialogues in average"
  },
  {
    "objectID": "DS002 Final Prez.html#limitations",
    "href": "DS002 Final Prez.html#limitations",
    "title": "Behind the Scenes",
    "section": "Limitations",
    "text": "Limitations\n\n“Negative” words are subjective! and only a very limited set of negative words were looked at here.\n\nLength of Acts were not captured: some acts may be much longer -&gt; leading to more negative words."
  },
  {
    "objectID": "DS002 Final Prez.html#what-about-love",
    "href": "DS002 Final Prez.html#what-about-love",
    "title": "Behind the Scenes",
    "section": "What about “love”???",
    "text": "What about “love”???\nDoes Macbeth ever verbally expressed “love” in the play??? and to whom?\n\nmacbeth |&gt;\n  filter( character == \"Macbeth\" ) |&gt;\n  mutate( love_strings = str_extract( dialogue, \"(?&lt;=(?i)love ).+\") ) |&gt;\n  filter( !is.na(love_strings) ) |&gt;\n  select( character:love_strings, -line_number )\n\n# A tibble: 4 × 3\n  character dialogue                                             love_strings   \n  &lt;chr&gt;     &lt;chr&gt;                                                &lt;chr&gt;          \n1 Macbeth   Safe toward your love and honour.                    and honour.    \n2 Macbeth   Courage to make 's love kno wn?                      kno wn?        \n3 Macbeth   Grapples you to the heart and love of us,            of us,         \n4 Macbeth   To those that know me. Come, love and health to all; and health to …\n\n\nSeems like it only occurred 4 times in the whole play and they seem to occur only in generic settings (“love and honour”, “love and health to all”, seems like words said when toasting) — poor lady Macbeth :("
  },
  {
    "objectID": "DS002 Final Prez.html#references",
    "href": "DS002 Final Prez.html#references",
    "title": "Behind the Scenes",
    "section": "References",
    "text": "References\nPicture taken from: https://medium.com/literary-analyses/lady-macbeth-30e549b7c211 Data taken from TidyTuesday\nGitHub Repository: https://github.com/rfordatascience/tidytuesday/tree/main/data\nData originally sourced from: https://shakespeare.mit.edu/"
  },
  {
    "objectID": "Strings.html#the-data",
    "href": "Strings.html#the-data",
    "title": "Macbeth",
    "section": "The Data",
    "text": "The Data\n“macbeth” from TidyTuesday!\n\n\nCode\nlibrary(dplyr)\nlibrary(tidyverse)\n\nmacbeth &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-17/macbeth.csv')\n\nhead(macbeth)\n\n\n# A tibble: 6 × 5\n  act   scene   character         dialogue                           line_number\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;             &lt;chr&gt;                                    &lt;dbl&gt;\n1 Act I Scene I [stage direction] Thunder and lightning. Enter thre…          NA\n2 Act I Scene I First Witch       When shall we three meet again               1\n3 Act I Scene I First Witch       In thunder, lightning, or in rain?           2\n4 Act I Scene I Second Witch      When the hurlyburly's done,                  3\n5 Act I Scene I Second Witch      When the battle's lost and won.              4\n6 Act I Scene I Third Witch       That will be ere the set of sun.             5\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(tidyverse)\n\nmacbeth &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-17/macbeth.csv')\n\nneg_lines &lt;- macbeth |&gt;\n  mutate( blood = str_detect( dialogue, \"(?i)blood|(?i)murder|(?i)death|(?i)kill\" ) ) |&gt;\n  filter(blood) |&gt;\n  group_by( act ) |&gt;\n  summarize( negative_lines_per_act = n() ) \n\naverage_character &lt;- macbeth |&gt;\n  mutate( string_length = str_length( dialogue ) ) |&gt;\n  group_by( act ) |&gt;\n  summarize( average_character = mean( string_length ) ) \n\nnegative_words &lt;- left_join( neg_lines, average_character )\n\np1 &lt;- ggplot( negative_words, aes(x = average_character, y = negative_lines_per_act, label = act)) +\n  geom_point(color = \"blue\", size = 3) +  \n  geom_text(vjust = -1, size = 3) +       \n  labs(title = \"\",\n       x = \"\",\n       y = \"\" ) +\n  theme_minimal()\n\n\n\n\nGet Average Negative Lines\nneg_lines &lt;- macbeth |&gt;\n  mutate( blood = str_detect( dialogue, \"(?i)blood|(?i)murder|(?i)death|(?i)kill\" ) ) |&gt;\n  group_by( act ) |&gt;\n  summarize( negative_lines_per_act = mean(blood) ) \n\nnegative_words &lt;- left_join( neg_lines, average_character )\n\np2 &lt;- ggplot( negative_words, aes(x = average_character, y = negative_lines_per_act, label = act)) +\n  geom_point(color = \"blue\", size = 3) +  \n  geom_text(vjust = -1, size = 3) +       \n  labs(title = \"\",\n       x = \"\",\n       y = \"\" ) +\n  theme_minimal()\n\n\n\n\nDefine Negative Words Differently\nneg_lines &lt;- macbeth |&gt;\n  mutate( blood = str_detect( dialogue, \"(?i)fight|(?i)hurt|(?i)die|(?i)dead\" ) ) |&gt;\n  filter(blood) |&gt;\n  group_by( act ) |&gt;\n  summarize( negative_lines_per_act = n() ) \n\nnegative_words &lt;- left_join( neg_lines, average_character )\n\np3 &lt;- ggplot( negative_words, aes(x = average_character, y = negative_lines_per_act, label = act)) +\n  geom_point(color = \"blue\", size = 3) +  \n  geom_text(vjust = -1, size = 3) +       \n  labs(title = \"\",\n       x = \"\",\n       y = \"\" ) +\n  theme_minimal()\n\n\n\n\nCode\nmacbeth |&gt;\n  filter( character == \"Macbeth\" ) |&gt;\n  mutate( love_strings = str_extract( dialogue, \"(?&lt;=(?i)love ).+\") ) |&gt;\n  filter( !is.na(love_strings) ) |&gt;\n  select( character:love_strings, -line_number )\n\n\n# A tibble: 4 × 3\n  character dialogue                                             love_strings   \n  &lt;chr&gt;     &lt;chr&gt;                                                &lt;chr&gt;          \n1 Macbeth   Safe toward your love and honour.                    and honour.    \n2 Macbeth   Courage to make 's love kno wn?                      kno wn?        \n3 Macbeth   Grapples you to the heart and love of us,            of us,         \n4 Macbeth   To those that know me. Come, love and health to all; and health to …\n\n\nAfter looking at the negative, I decided to turn to the opposite — whether if Macbeth, the main character of the play, has explicity expressed “love” in the play, and to whom. So I used the lookaround function to spot instances of Macbeth saying the word “love” and the “love_strings” is whatever comes after “love”. Sadly, these instances only occured 4 times in the whole play and they seem to occur only in more generic settings (like toasting) — poor lady Macbeth :(\nData taken from TidyTuesday GitHub Repository: https://github.com/rfordatascience/tidytuesday/tree/main/data\nData originally sourced from:\nhttps://shakespeare.mit.edu/"
  },
  {
    "objectID": "Strings.html#lets-look-at-the-data",
    "href": "Strings.html#lets-look-at-the-data",
    "title": "Macbeth",
    "section": "Let’s Look at the Data",
    "text": "Let’s Look at the Data\n\n# Load required libraries\nlibrary(dplyr)\nlibrary(tidyverse)\n\n# Read the Macbeth dialogue data\nmacbeth &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-17/macbeth.csv')\n\n# Display first few rows\nhead(macbeth)\n\n# A tibble: 6 × 5\n  act   scene   character         dialogue                           line_number\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;             &lt;chr&gt;                                    &lt;dbl&gt;\n1 Act I Scene I [stage direction] Thunder and lightning. Enter thre…          NA\n2 Act I Scene I First Witch       When shall we three meet again               1\n3 Act I Scene I First Witch       In thunder, lightning, or in rain?           2\n4 Act I Scene I Second Witch      When the hurlyburly's done,                  3\n5 Act I Scene I Second Witch      When the battle's lost and won.              4\n6 Act I Scene I Third Witch       That will be ere the set of sun.             5\n\n\nEach observation in this dataset is a line of dialogue (excluding stage directions) from a character in the play. The dataset is arranged in the same order as the original script."
  },
  {
    "objectID": "Strings.html#lets-look-at-the-data-1",
    "href": "Strings.html#lets-look-at-the-data-1",
    "title": "Macbeth",
    "section": "Lets Look at The Data",
    "text": "Lets Look at The Data\n\nmacbeth &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-17/macbeth.csv')\n\nneg_lines &lt;- macbeth |&gt;\n  mutate( blood = str_detect( dialogue, \"(?i)blood|(?i)murder|(?i)death|(?i)kill\" ) ) |&gt;\n  filter(blood) |&gt;\n  group_by( act ) |&gt;\n  summarize( negative_lines_per_act = n() ) \n\naverage_character &lt;- macbeth |&gt;\n  mutate( string_length = str_length( dialogue ) ) |&gt;\n  group_by( act ) |&gt;\n  summarize( average_character = mean( string_length ) ) \n\nnegative_words &lt;- left_join( neg_lines, average_character )\n\np1 &lt;- ggplot( negative_words, aes(x = average_character, y = negative_lines_per_act, label = act)) +\n  geom_point(color = \"blue\", size = 3) +  \n  geom_text(vjust = -1, size = 3) +       \n  labs(title = \"\",\n       x = \"\",\n       y = \"\" ) +\n  theme_minimal()\n\n\n\nGet Average Negative Lines\nneg_lines &lt;- macbeth |&gt;\n  mutate( blood = str_detect( dialogue, \"(?i)blood|(?i)murder|(?i)death|(?i)kill\" ) ) |&gt;\n  group_by( act ) |&gt;\n  summarize( negative_lines_per_act = mean(blood) ) \n\nnegative_words &lt;- left_join( neg_lines, average_character )\n\np2 &lt;- ggplot( negative_words, aes(x = average_character, y = negative_lines_per_act, label = act)) +\n  geom_point(color = \"blue\", size = 3) +  \n  geom_text(vjust = -1, size = 3) +       \n  labs(title = \"\",\n       x = \"\",\n       y = \"\" ) +\n  theme_minimal()\n\n\n\n\nDefine Negative Words Differently\nneg_lines &lt;- macbeth |&gt;\n  mutate( blood = str_detect( dialogue, \"(?i)fight|(?i)hurt|(?i)die|(?i)dead\" ) ) |&gt;\n  filter(blood) |&gt;\n  group_by( act ) |&gt;\n  summarize( negative_lines_per_act = n() ) \n\nnegative_words &lt;- left_join( neg_lines, average_character )\n\np3 &lt;- ggplot( negative_words, aes(x = average_character, y = negative_lines_per_act, label = act)) +\n  geom_point(color = \"blue\", size = 3) +  \n  geom_text(vjust = -1, size = 3) +       \n  labs(title = \"\",\n       x = \"\",\n       y = \"\" ) +\n  theme_minimal()\n\n\n\nmacbeth |&gt;\n  filter( character == \"Macbeth\" ) |&gt;\n  mutate( love_strings = str_extract( dialogue, \"(?&lt;=(?i)love ).+\") ) |&gt;\n  filter( !is.na(love_strings) ) |&gt;\n  select( character:love_strings, -line_number )\n\n# A tibble: 4 × 3\n  character dialogue                                             love_strings   \n  &lt;chr&gt;     &lt;chr&gt;                                                &lt;chr&gt;          \n1 Macbeth   Safe toward your love and honour.                    and honour.    \n2 Macbeth   Courage to make 's love kno wn?                      kno wn?        \n3 Macbeth   Grapples you to the heart and love of us,            of us,         \n4 Macbeth   To those that know me. Come, love and health to all; and health to …\n\n\nAfter looking at the negative, I decided to turn to the opposite — whether if Macbeth, the main character of the play, has explicity expressed “love” in the play, and to whom. So I used the lookaround function to spot instances of Macbeth saying the word “love” and the “love_strings” is whatever comes after “love”. Sadly, these instances only occured 4 times in the whole play and they seem to occur only in more generic settings (like toasting) — poor lady Macbeth :(\nData taken from TidyTuesday GitHub Repository: https://github.com/rfordatascience/tidytuesday/tree/main/data\nData originally sourced from:\nhttps://shakespeare.mit.edu/"
  },
  {
    "objectID": "Strings.html#negative",
    "href": "Strings.html#negative",
    "title": "Macbeth",
    "section": "Negative",
    "text": "Negative\n\nmacbeth &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-17/macbeth.csv')\n\nneg_lines &lt;- macbeth |&gt;\n  mutate( blood = str_detect( dialogue, \"(?i)blood|(?i)murder|(?i)death|(?i)kill\" ) ) |&gt;\n  filter(blood) |&gt;\n  group_by( act ) |&gt;\n  summarize( negative_lines_per_act = n() ) \n\naverage_character &lt;- macbeth |&gt;\n  mutate( string_length = str_length( dialogue ) ) |&gt;\n  group_by( act ) |&gt;\n  summarize( average_character = mean( string_length ) ) \n\nnegative_words &lt;- left_join( neg_lines, average_character )\n\np1 &lt;- ggplot( negative_words, aes(x = average_character, y = negative_lines_per_act, label = act)) +\n  geom_point(color = \"blue\", size = 3) +  \n  geom_text(vjust = -1, size = 3) +       \n  labs(title = \"\",\n       x = \"\",\n       y = \"\" ) +\n  theme_minimal()\n\n\n\nGet Average Negative Lines\nneg_lines &lt;- macbeth |&gt;\n  mutate( blood = str_detect( dialogue, \"(?i)blood|(?i)murder|(?i)death|(?i)kill\" ) ) |&gt;\n  group_by( act ) |&gt;\n  summarize( negative_lines_per_act = mean(blood) ) \n\nnegative_words &lt;- left_join( neg_lines, average_character )\n\np2 &lt;- ggplot( negative_words, aes(x = average_character, y = negative_lines_per_act, label = act)) +\n  geom_point(color = \"blue\", size = 3) +  \n  geom_text(vjust = -1, size = 3) +       \n  labs(title = \"\",\n       x = \"\",\n       y = \"\" ) +\n  theme_minimal()\n\n\n\n\nDefine Negative Words Differently\nneg_lines &lt;- macbeth |&gt;\n  mutate( blood = str_detect( dialogue, \"(?i)fight|(?i)hurt|(?i)die|(?i)dead\" ) ) |&gt;\n  filter(blood) |&gt;\n  group_by( act ) |&gt;\n  summarize( negative_lines_per_act = n() ) \n\nnegative_words &lt;- left_join( neg_lines, average_character )\n\np3 &lt;- ggplot( negative_words, aes(x = average_character, y = negative_lines_per_act, label = act)) +\n  geom_point(color = \"blue\", size = 3) +  \n  geom_text(vjust = -1, size = 3) +       \n  labs(title = \"\",\n       x = \"\",\n       y = \"\" ) +\n  theme_minimal()\n\n\n\nmacbeth |&gt;\n  filter( character == \"Macbeth\" ) |&gt;\n  mutate( love_strings = str_extract( dialogue, \"(?&lt;=(?i)love ).+\") ) |&gt;\n  filter( !is.na(love_strings) ) |&gt;\n  select( character:love_strings, -line_number )\n\n# A tibble: 4 × 3\n  character dialogue                                             love_strings   \n  &lt;chr&gt;     &lt;chr&gt;                                                &lt;chr&gt;          \n1 Macbeth   Safe toward your love and honour.                    and honour.    \n2 Macbeth   Courage to make 's love kno wn?                      kno wn?        \n3 Macbeth   Grapples you to the heart and love of us,            of us,         \n4 Macbeth   To those that know me. Come, love and health to all; and health to …\n\n\nAfter looking at the negative, I decided to turn to the opposite — whether if Macbeth, the main character of the play, has explicity expressed “love” in the play, and to whom. So I used the lookaround function to spot instances of Macbeth saying the word “love” and the “love_strings” is whatever comes after “love”. Sadly, these instances only occured 4 times in the whole play and they seem to occur only in more generic settings (like toasting) — poor lady Macbeth :(\nData taken from TidyTuesday GitHub Repository: https://github.com/rfordatascience/tidytuesday/tree/main/data\nData originally sourced from:\nhttps://shakespeare.mit.edu/"
  },
  {
    "objectID": "Strings.html#negative-words-vs-string-length",
    "href": "Strings.html#negative-words-vs-string-length",
    "title": "Macbeth",
    "section": "Negative Words VS String Length",
    "text": "Negative Words VS String Length\n\nmacbeth &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-17/macbeth.csv')\n\nneg_lines &lt;- macbeth |&gt;\n  mutate( blood = str_detect( dialogue, \"(?i)blood|(?i)murder|(?i)death|(?i)kill\" ) ) |&gt;\n  filter(blood) |&gt;\n  group_by( act ) |&gt;\n  summarize( negative_lines_per_act = n() ) \n\naverage_character &lt;- macbeth |&gt;\n  mutate( string_length = str_length( dialogue ) ) |&gt;\n  group_by( act ) |&gt;\n  summarize( average_character = mean( string_length ) ) \n\nnegative_words &lt;- left_join( neg_lines, average_character )\n\nggplot( negative_words, aes(x = average_character, y = negative_lines_per_act, label = act)) +\n  geom_point(color = \"blue\", size = 3) +  \n  geom_text(vjust = -1, size = 3) +       \n  labs(x = \"Average String Length\",\n       y = \"Total Negative Lines\" ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nI defined “negative words” to be set of words containing “blood”, “murder”, “death” and “kill”, and I counted the number of lines that contained these words in each act, I called them negative lines. I also calculated the average string length (which is the length of the lines) in each act and examined the relationship between the number of negative lines and the length of the lines in each act. This plot shows that the relationship is somewhat negative, which could be translated to mean that characters tend to use shorter sentences in the acts that have more negative lines. However, maybe some acts are just much longer and contains more dialogues from characters, therefore raising the total number of negative lines. Let’s see if anything changes if we instead went for the average number of negative lines per act.\n\n\nGet Average Negative Lines\nneg_lines &lt;- macbeth |&gt;\n  mutate( blood = str_detect( dialogue, \"(?i)blood|(?i)murder|(?i)death|(?i)kill\" ) ) |&gt;\n  group_by( act ) |&gt;\n  summarize( avg_negative_lines_per_act = mean(blood) ) \n\nnegative_words &lt;- left_join( neg_lines, average_character )\n\nggplot( negative_words, aes(x = average_character, y = avg_negative_lines_per_act, label = act)) +\n  geom_point(color = \"blue\", size = 3) +  \n  geom_text(vjust = -1, size = 3) +       \n  labs(x = \"Average String Length\",\n       y = \"Average Negative Lines\" ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nHey, it’s not significantly different from the previous graph, which means the length of the acts did not affect our investigation. Namely, Act II is still really high up there despite controlling for the length of the act: matches our intuition! That’s when Macbeth murdered the king!\nHere’s another problem. The “negative words” are a set of completely arbitrary words. Would the outcome change if we defined “negative words” differently?\n\n\nDefine Negative Words Differently\nneg_lines &lt;- macbeth |&gt;\n  mutate( blood = str_detect( dialogue, \"(?i)fight|(?i)hurt|(?i)die|(?i)dead\" ) ) |&gt;\n  filter(blood) |&gt;\n  group_by( act ) |&gt;\n  summarize( negative_lines_per_act = n() ) \n\nnegative_words &lt;- left_join( neg_lines, average_character )\n\nggplot( negative_words, aes(x = average_character, y = negative_lines_per_act, label = act)) +\n  geom_point(color = \"blue\", size = 3) +  \n  geom_text(vjust = -1, size = 3) +       \n  labs(x = \"Average String Length\",\n       y = \"Average Negative Lines\" ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nYes, indeed! Instead of (blood, murder, death, kill), the new set of negative words is defined as (fight, hurt, die, dead). We see completely different trends. Now, Act V is on the top for containing the highest number of negative lines on average. Act V is when most of the main characters, including Macbeth and Lady Macbeth, die after the final battle broke out. This intuitively matches this new set of “negative words”."
  },
  {
    "objectID": "Strings.html#loving-strings",
    "href": "Strings.html#loving-strings",
    "title": "Macbeth",
    "section": "Loving Strings",
    "text": "Loving Strings\n\n# Filter for lines by Macbeth containing \"love\", and extract what comes after \"love\"\nmacbeth |&gt;\n  filter(character == \"Macbeth\") |&gt;\n  mutate(love_strings = str_extract(dialogue, \"(?&lt;=(?i)love ).+\")) |&gt;\n  filter(!is.na(love_strings)) |&gt;\n  select(character:love_strings, -line_number)\n\n# A tibble: 4 × 3\n  character dialogue                                             love_strings   \n  &lt;chr&gt;     &lt;chr&gt;                                                &lt;chr&gt;          \n1 Macbeth   Safe toward your love and honour.                    and honour.    \n2 Macbeth   Courage to make 's love kno wn?                      kno wn?        \n3 Macbeth   Grapples you to the heart and love of us,            of us,         \n4 Macbeth   To those that know me. Come, love and health to all; and health to …\n\n\nAfter analyzing dark themes, I wanted to explore the opposite. Has Macbeth ever spoken about love? I filtered the data to include only lines spoken by Macbeth and used a lookaround regular expression to extract any text that follows the word “love”. Disappointingly, such expressions occurred only four times—and most were general in tone (e.g., during toasts), rather than personal or romantic. Poor Lady Macbeth.\n\nData Citations\nTidyTuesday Dataset URL: https://github.com/rfordatascience/tidytuesday/tree/main/data/2024/2024-09-17 Author: TidyTuesday | Community Title: Macbeth Dialogue | Dataset Publication Source: GitHub (R for Data Science Learning Community), 2024\nOriginal Text Source URL: https://shakespeare.mit.edu/macbeth/full.html Editor: MIT Shakespeare | Title: The Complete Works of William Shakespeare | Publication Source: MIT, Public Domain\n\n\nData Origin and Purpose\nThe data was collected and shared by the R4DS (R for Data Science) Online Learning Community as part of their weekly TidyTuesday project. These datasets are curated to help learners practice data wrangling, visualization, and storytelling using real-world datasets. The Shakespeare dataset was sourced from MIT’s public domain archive and restructured into a tabular format for easier analysis and visualization."
  },
  {
    "objectID": "Strings.html#negative-words-vs.-string-length",
    "href": "Strings.html#negative-words-vs.-string-length",
    "title": "Macbeth",
    "section": "Negative Words vs. String Length",
    "text": "Negative Words vs. String Length\n\n# Count lines per act containing \"negative\" words\nneg_lines &lt;- macbeth |&gt;\n  mutate(blood = str_detect(dialogue, \"(?i)blood|(?i)murder|(?i)death|(?i)kill\")) |&gt;\n  filter(blood) |&gt;\n  group_by(act) |&gt;\n  summarize(negative_lines_per_act = n())\n\n# Compute average string length per act\naverage_character &lt;- macbeth |&gt;\n  mutate(string_length = str_length(dialogue)) |&gt;\n  group_by(act) |&gt;\n  summarize(average_character = mean(string_length))\n\n# Merge summaries\nnegative_words &lt;- left_join(neg_lines, average_character)\n\n# Plot relationship\nggplot(negative_words, aes(x = average_character, y = negative_lines_per_act, label = act)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_text(vjust = -1, size = 3) +\n  labs(\n    x = \"Average String Length\",\n    y = \"Total Negative Lines\",\n    title = \"Negative Lines vs. Average String Length by Act\",\n    alt = \"A scatter plot with average dialogue length on the x-axis and number of negative lines on the y-axis, labeled by act\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1. Negative Lines vs. Average String Length by Act This scatter plot explores whether acts with more negatively worded lines (blood, murder, death, kill) tend to feature shorter or longer lines on average. It appears there’s a slight negative trend—suggesting that darker scenes may include shorter dialogue.\nI defined negative words as any line containing “blood”, “murder”, “death”, or “kill”. I then counted the number of such lines in each act and compared that count to the average length of lines in the same act. This plot shows a slight negative relationship, which might imply that characters use shorter sentences in darker scenes. However, it could also be that some acts are longer overall and have more dialogue, which inflates the negative line count. Let’s control for act length next."
  },
  {
    "objectID": "Strings.html#average-negative-lines-per-act",
    "href": "Strings.html#average-negative-lines-per-act",
    "title": "Macbeth",
    "section": "Average Negative Lines per Act",
    "text": "Average Negative Lines per Act\n\n\nGet Average Negative Lines\n# Calculate average presence of negative words per act\nneg_lines &lt;- macbeth |&gt;\n  mutate(blood = str_detect(dialogue, \"(?i)blood|(?i)murder|(?i)death|(?i)kill\")) |&gt;\n  group_by(act) |&gt;\n  summarize(avg_negative_lines_per_act = mean(blood))\n\n# Join with average string length data\nnegative_words &lt;- left_join(neg_lines, average_character)\n\n# Plot\nggplot(negative_words, aes(x = average_character, y = avg_negative_lines_per_act, label = act)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_text(vjust = -1, size = 3) +\n  labs(\n    x = \"Average String Length\",\n    y = \"Average Negative Lines (Proportion)\",\n    title = \"Proportion of Negative Lines vs. Average String Length\",\n    alt = \"A scatter plot with average dialogue length on the x-axis and proportion of negative lines on the y-axis, labeled by act.\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nFigure 2. Proportion of Negative Lines vs. Average String Length This plot shows that even after accounting for the number of lines in each act (by using proportions), the trend remains similar. Act II still stands out as the most negative—consistent with the murder of King Duncan.\nBy calculating the average rate of negative lines (as a proportion), we control for act length. The trend remains consistent, especially with Act II still exhibiting a high proportion of negativity—aligning with when Macbeth murders the king."
  },
  {
    "objectID": "Strings.html#defining-negative-words-differently",
    "href": "Strings.html#defining-negative-words-differently",
    "title": "Macbeth",
    "section": "Defining Negative Words Differently",
    "text": "Defining Negative Words Differently\n\n\nDefine Negative Words Differently\n# New negative words: fight, hurt, die, dead\nneg_lines &lt;- macbeth |&gt;\n  mutate(blood = str_detect(dialogue, \"(?i)fight|(?i)hurt|(?i)die|(?i)dead\")) |&gt;\n  filter(blood) |&gt;\n  group_by(act) |&gt;\n  summarize(negative_lines_per_act = n())\n\n# Merge with string length\nnegative_words &lt;- left_join(neg_lines, average_character)\n\n# Plot\nggplot(negative_words, aes(x = average_character, y = negative_lines_per_act, label = act)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_text(vjust = -1, size = 3) +\n  labs(\n    x = \"Average String Length\",\n    y = \"Total Negative Lines\",\n    title = \"Negative Lines Using Alternate Word Set\",\n    alt = \"A scatter plot comparing acts based on average dialogue length and number of lines with the alternate set of negative words.\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nFigure 3. Negative Lines Using Alternate Word Set When redefining “negative” to mean lines that include “fight”, “hurt”, “die”, or “dead”, we see a different pattern. Act V now appears as the most negative—reflecting the climactic violence and deaths in the final scenes.\nBy redefining negative language, the results shift. Act V now tops the list for negativity, which makes sense—this is where the final battle takes place and multiple major characters, including Macbeth and Lady Macbeth, meet their end."
  },
  {
    "objectID": "Ethics.html#scenario",
    "href": "Ethics.html#scenario",
    "title": "Ethics in Data Science",
    "section": "",
    "text": "The COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) algorithm was developed by a for-profit company called Northpointe. It is used in courtrooms across the U.S. to help inform sentencing decisions, set bail/bond amounts, and assist with other decision-making in the criminal justice system (Angwin et al., 2016). The algorithm’s key feature is to produce “risk ratings” for defendants based on their criminal history and their responses to a questionnaire of more than 100 questions (Angwin et al., 2016). A major ethical concern is that the algorithm appears to be racially and gender sensitive. It was found to be “particularly likely to falsely flag Black defendants as future criminals, wrongly labeling them this way at almost twice the rate of white defendants” (Angwin et al., 2016), and that “women were rated two classes worse than men” (Drosser et al., 2020).\nIt is easy to conclude that the algorithm is biased—or even racist or sexist—against defendants, but the issue may be deeper than that. For one thing, the algorithm seems accurate in predicting recidivism rates across races. In other words, there is a similar proportion of white defendants who will re-offend compared to Black defendants with the same risk rating (Corbett-Davies et al., 2018), and that appears to be what the risk ratings were initially designed to reflect. It has been proposed that maybe it is impossible for an algorithm to achieve both:\n\nWell-calibrated risk between groups (where risk ratings mean the same thing across groups), and\nEqual false positive rates (not falsely flagging certain groups more than others).\n\nMaybe this is achievable in a perfect society where everyone receives equal resources, education, etc. But in our current world—where inequality and systemic discrimination are common, especially due to historical factors—racial groups may unfortunately correlate with the likelihood to commit (or recommit) a crime. This is known as base rate differences. That correlation may be the reason why it’s impossible to achieve both fairness criteria at once. (This argument is mainly derived from Corbett-Davies et al., 2018.)"
  },
  {
    "objectID": "Ethics.html#what-was-the-data-collection-process-were-the-observations-collected-ethically-are-there-missing-observations",
    "href": "Ethics.html#what-was-the-data-collection-process-were-the-observations-collected-ethically-are-there-missing-observations",
    "title": "Ethics in Data Science",
    "section": "What was the data collection process? Were the observations collected ethically? Are there missing observations?",
    "text": "What was the data collection process? Were the observations collected ethically? Are there missing observations?\nAn issue with the data collection process could be that data is acquired without the subject’s consent—or even their knowledge. Applying this to the example above is a bit tricky, because the subjects are suspects in criminal cases. In the algorithm, data are drawn from defendants’ criminal records (probably automatically), and the defendants are given some questions to answer. The fact that a private company can access someone’s criminal records may or may not be an issue, but some of the questions do seem problematic. For example, one question asks whether the defendant agrees or disagrees with statements like:\n\n“A hungry person has a right to steal,” and “If people make me angry or lose my temper, I can be dangerous.” (Angwin et al., 2016)"
  },
  {
    "objectID": "Ethics.html#is-the-data-being-used-in-unintended-ways-to-the-original-study",
    "href": "Ethics.html#is-the-data-being-used-in-unintended-ways-to-the-original-study",
    "title": "Ethics in Data Science",
    "section": "Is the data being used in unintended ways to the original study?",
    "text": "Is the data being used in unintended ways to the original study?\nYes. The algorithm was invented by Tim Brennan, a former professor of statistics at the University of Colorado. He later sold it to the for-profit company that currently runs it. Brennan claimed that his original focus was for the algorithm to “reduce crime rather than punish,” and he “does not like the idea that COMPAS [is] the sole evidence that a decision would be based upon” (Angwin et al., 2016). But whether he likes it or not, the algorithm has since been adopted by many courts and agencies in the criminal justice system across the country—and its use has definitely departed from its original intent."
  },
  {
    "objectID": "Ethics.html#should-race-be-used-as-a-variable-is-it-a-proxy-for-something-else-what-about-gender",
    "href": "Ethics.html#should-race-be-used-as-a-variable-is-it-a-proxy-for-something-else-what-about-gender",
    "title": "Ethics in Data Science",
    "section": "Should race be used as a variable? Is it a proxy for something else? What about gender?",
    "text": "Should race be used as a variable? Is it a proxy for something else? What about gender?\nI think whether race is explicitly used as a variable or not, many algorithms—COMPAS included—still effectively use race through proxy attributes (the same goes for gender). Interestingly, according to Northpointe, race, gender, or any other protected social categories are not explicitly asked about or entered into the algorithm (Angwin et al., 2016). However, there are many reasons to believe the algorithm uses other proxies for race and gender. For instance, the algorithm appears to rate female defendants as “riskier” than male defendants by two full risk levels, according to one study (Drosser et al., 2020)."
  },
  {
    "objectID": "Ethics.html#did-it-create-reproducible-and-extensible-work",
    "href": "Ethics.html#did-it-create-reproducible-and-extensible-work",
    "title": "Ethics in Data Science",
    "section": "Did it create reproducible and extensible work?",
    "text": "Did it create reproducible and extensible work?\nIt did not. The questions and calculations are mostly not disclosed to the public due to proprietary concerns, making it nearly impossible to reproduce the calculations or obtain the same risk ratings (Angwin et al., 2016). More importantly, defendants “rarely have the opportunity to challenge their assessments” because the calculation methods and logic behind the ratings are not revealed. This may be unfair to defendants and could be particularly harmful if the algorithm produces an inaccurate rating."
  },
  {
    "objectID": "Ethics.html#citations",
    "href": "Ethics.html#citations",
    "title": "Ethics in Data Science",
    "section": "Citations",
    "text": "Citations\n\nAngwin, J., Larson, J., Mattu, S., & Kirchner, L. (2016, May 23). Machine bias. ProPublica. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\nCorbett-Davies, S., & Goel, S. (2018, October 17). A computer program used for bail and sentencing decisions was labeled biased against blacks. It’s actually not that clear. The Washington Post. https://www.washingtonpost.com/news/monkey-cage/wp/2018/10/17/a-computer-program-used-for-bail-and-sentencing-decisions-was-labeled-biased-against-blacks-its-actually-not-that-clear/\nDrosser, C. (2020, August 5). In order not to discriminate, we might have to discriminate. The New York Times. https://www.nytimes.com/2020/08/05/magazine/algorithm-discrimination.html"
  },
  {
    "objectID": "The Scent of Data.html#my-shiny-code",
    "href": "The Scent of Data.html#my-shiny-code",
    "title": "The Scent of Data",
    "section": "My Shiny Code",
    "text": "My Shiny Code\nlibrary(shiny) library(tidyverse)\n\nLoad the data\nurl &lt;- “https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-12-10/parfumo_data_clean.csv” perfume_data &lt;- read_csv(url)\n\n\nUI\nui &lt;- fluidPage(\ntitlePanel(“Perfume Rating Distribution”), sidebarLayout( sidebarPanel( selectInput(“selected_perfume”, “Choose a Perfume Brand:”, choices = sort(unique(perfume_data$Brand))) ), mainPanel( plotOutput(“rating_plot”) ) ) )\n\n\nServer\nserver &lt;- function(input, output) {\nselected_data &lt;- reactive({ perfume_data %&gt;% filter(Brand == input$selected_perfume) })\noutput\\(rating_plot &lt;- renderPlot({\n    ggplot(perfume_data, aes(y = Rating_Value, x = \"\")) +\n      geom_boxplot(fill = \"lightblue\", outlier.shape = NA) +\n      geom_point(data = selected_data(),\n                 aes(y = Rating_Value),\n                 color = \"red\", size = 3) +\n      labs(title = paste(\"Rating Distribution (with\", input\\)selected_perfume, “highlighted)”), y = “Rating Value”, x = ““) + theme_minimal() }) }\n\n\nRun the app\nshinyApp(ui = ui, server = server)\nData taken from TidyTuesday GitHub Repository: https://github.com/rfordatascience/tidytuesday/tree/main/data\nData originally sourced from Perfumo:\nhttps://www.parfumo.com/"
  },
  {
    "objectID": "The Scent of Data.html#citations",
    "href": "The Scent of Data.html#citations",
    "title": "The Scent of Data",
    "section": "Citations",
    "text": "Citations\n\nOlga G. Parfumo Fragrance Dataset. Kaggle. 2021. Available at: https://www.kaggle.com/datasets/olgagmiufana1/perfume-dataset\nR4DS TidyTuesday Project. TidyTuesday Datasets. GitHub repository. 2024. Available at: https://github.com/rfordatascience/tidytuesday\nParfumo. Perfume Community and Reviews. https://www.parfumo.com/"
  },
  {
    "objectID": "The Scent of Data.html#who-collected-the-data-and-why",
    "href": "The Scent of Data.html#who-collected-the-data-and-why",
    "title": "The Scent of Data",
    "section": "Who Collected the Data and Why?",
    "text": "Who Collected the Data and Why?\nThe Parfumo dataset was initially scraped and compiled by Olga G., a Kaggle user, for the purpose of enabling data analysis and modeling related to perfume products. The dataset includes information on perfume names, brands, accords, release years, and community ratings. It was intended for public use in machine learning, data visualization, and statistical exploration tasks. By publishing the dataset on Kaggle, Olga contributed to the open data community and made it accessible for educational and research purposes.\nThe dataset was later included in the TidyTuesday project, a weekly initiative by the R for Data Science (R4DS) community to encourage practice in data wrangling and visualization using real-world datasets."
  },
  {
    "objectID": "Childcare Costs.html#citations",
    "href": "Childcare Costs.html#citations",
    "title": "Childcare Costs",
    "section": "Citations",
    "text": "Citations\n\nMock, Thomas. TidyTuesday: Childcare Costs Dataset. GitHub - rfordatascience/tidytuesday. 2023. Available at: https://github.com/rfordatascience/tidytuesday/tree/main/data/2023/2023-05-09\nU.S. Department of Labor, Women’s Bureau. National Database of Childcare Prices (NDCP). 2022. Available at: https://www.dol.gov/agencies/wb/topics/featured-childcare"
  },
  {
    "objectID": "SQL.html#does-the-total-amount-of-traffic-stops-differ-based-on-race.",
    "href": "SQL.html#does-the-total-amount-of-traffic-stops-differ-based-on-race.",
    "title": "Stanford Open Policing Project",
    "section": "Does the total amount of traffic stops differ based on race.",
    "text": "Does the total amount of traffic stops differ based on race.\n\nSELECT \n  subject_race, \n  COUNT(*) AS n_stops\nFROM la_new_orleans_2020_04_01\nWHERE type = 'vehicular'\n  AND subject_race IN ('White', 'Black')\n  AND STR_TO_DATE(CONCAT(SUBSTR(date, 1, 4), '-01-01'), '%Y-%m-%d') = '2018-01-01'\nGROUP BY subject_race\nORDER BY n_stops DESC;\n\n\n2 records\n\n\nsubject_race\nn_stops\n\n\n\n\nblack\n16087\n\n\nwhite\n4389\n\n\n\n\n\nTo narrow down the focus and avoiding to deal with too much data, I decided to look at the total amount of vehicular traffic stops in New Orleans in 2018 separated by black or white subjects. Here I found that the amount of incidents where a black subjects is stopped on the road is almost four times the amount that of white subjects. Even though, according to online sources, African Americans makes up approximately 60% of the population in New Orleans in 2018, whereas Whites only 30%, it is still quite disproportional that Black subjects are stopped four times as much as white subjects.\n\ndoes the total amount of traffic stops differ over time.\n\n\n\nSELECT \n  SUBSTR(date, 1, 4) AS year,\n  COUNT(*) AS n_stops\nFROM la_new_orleans_2020_04_01\nWHERE type = 'vehicular'\n  AND STR_TO_DATE(CONCAT(vehicle_year, '-01-01'), '%Y-%m-%d') BETWEEN '2010-01-01' AND '2018-12-31'\nGROUP BY year\nORDER BY year;\n\n\n9 records\n\n\nyear\nn_stops\n\n\n\n\n2010\n550\n\n\n2011\n1983\n\n\n2012\n2407\n\n\n2013\n2311\n\n\n2014\n5764\n\n\n2015\n10238\n\n\n2016\n8931\n\n\n2017\n13515\n\n\n2018\n9184\n\n\n\n\n\nContinuing from the previous query, I decided to look at the amount of vehicular stops in New Orleans every year since 2010 to 2018. The first 6 years show a fairly consistent increase in number of stops, which I suspect that it has to do with increased ownership of cars and population growth.\n\nwhat are some most common reasons that warrants the stops, and whether it differs based on the type of stops (i.e vehicular or pedestrian).\n\n\n\nSELECT \n  type,\n  reason_for_stop,\n  COUNT(*) AS n_stops\nFROM la_new_orleans_2020_04_01\nWHERE type IN ('pedestrian', 'vehicular')\nGROUP BY type, reason_for_stop\nORDER BY type, n_stops DESC;\n\n\n9 records\n\n\n\n\n\n\n\ntype\nreason_for_stop\nn_stops\n\n\n\n\npedestrian\nSUSPECT PERSON\n63829\n\n\nvehicular\nTRAFFIC VIOLATION\n292318\n\n\nvehicular\nSUSPECT VEHICLE\n6030\n\n\nvehicular\nTRAFFIC VIOLATION|CALL FOR SERVICE\n2\n\n\nvehicular\nCALL FOR SERVICE|TRAFFIC VIOLATION\n2\n\n\nvehicular\nTRAFFIC VIOLATION|OTHER\n1\n\n\nvehicular\nSUSPECT VEHICLE|SUSPECT PERSON|SUSPECT PERSON\n1\n\n\nvehicular\nCRIMINAL VIOLATION|TRAFFIC VIOLATION\n1\n\n\nvehicular\nTRAFFIC VIOLATION|OTHER|OTHER\n1\n\n\n\n\n\nLastly, I looked at some common reasons behind the traffic stops in New Orleans over the total amount of time this data was collected. Interestingly, pedestrian stops are unanimously due to “suspect person” and nothing related to traffic while vehicular stops are almost certainly related to traffic misdemeanors.\n\ndbDisconnect(con_traffic, shutdown = TRUE)\n\nIn conclusion, I found that in New Orleans 2018, there was almost four times the amount of vehicular traffic stops that involved a black subject than white subject, these stops seem to be consistently increasing through 2010 to 2015, and pedestrian stops seem to be due to non-traffic reasons while vehicular stops are almost always related to traffic reasons for the amount of time this study has been carried out.\nReference.\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10."
  },
  {
    "objectID": "SQL.html#do",
    "href": "SQL.html#do",
    "title": "Stanford Open Policing Project",
    "section": "Do",
    "text": "Do\n\nSELECT \n  subject_race, \n  COUNT(*) AS n_stops\nFROM la_new_orleans_2020_04_01\nWHERE type = 'vehicular'\n  AND subject_race IN ('White', 'Black')\n  AND STR_TO_DATE(CONCAT(SUBSTR(date, 1, 4), '-01-01'), '%Y-%m-%d') = '2018-01-01'\nGROUP BY subject_race\nORDER BY n_stops DESC;\n\n\n2 records\n\n\nsubject_race\nn_stops\n\n\n\n\nblack\n16087\n\n\nwhite\n4389\n\n\n\n\n\nTo narrow down the focus and avoiding to deal with too much data, I decided to look at the total amount of vehicular traffic stops in New Orleans in 2018 separated by black or white subjects. Here I found that the amount of incidents where a black subjects is stopped on the road is almost four times the amount that of white subjects. Even though, according to online sources, African Americans makes up approximately 60% of the population in New Orleans in 2018, whereas Whites only 30%, it is still quite disproportional that Black subjects are stopped four times as much as white subjects.\n\ndoes the total amount of traffic stops differ over time.\n\n\n\nSELECT \n  SUBSTR(date, 1, 4) AS year,\n  COUNT(*) AS n_stops\nFROM la_new_orleans_2020_04_01\nWHERE type = 'vehicular'\n  AND STR_TO_DATE(CONCAT(vehicle_year, '-01-01'), '%Y-%m-%d') BETWEEN '2010-01-01' AND '2018-12-31'\nGROUP BY year\nORDER BY year;\n\n\n9 records\n\n\nyear\nn_stops\n\n\n\n\n2010\n550\n\n\n2011\n1983\n\n\n2012\n2407\n\n\n2013\n2311\n\n\n2014\n5764\n\n\n2015\n10238\n\n\n2016\n8931\n\n\n2017\n13515\n\n\n2018\n9184\n\n\n\n\n\nContinuing from the previous query, I decided to look at the amount of vehicular stops in New Orleans every year since 2010 to 2018. The first 6 years show a fairly consistent increase in number of stops, which I suspect that it has to do with increased ownership of cars and population growth.\n\nwhat are some most common reasons that warrants the stops, and whether it differs based on the type of stops (i.e vehicular or pedestrian).\n\n\n\nSELECT \n  type,\n  reason_for_stop,\n  COUNT(*) AS n_stops\nFROM la_new_orleans_2020_04_01\nWHERE type IN ('pedestrian', 'vehicular')\nGROUP BY type, reason_for_stop\nORDER BY type, n_stops DESC;\n\n\n9 records\n\n\n\n\n\n\n\ntype\nreason_for_stop\nn_stops\n\n\n\n\npedestrian\nSUSPECT PERSON\n63829\n\n\nvehicular\nTRAFFIC VIOLATION\n292318\n\n\nvehicular\nSUSPECT VEHICLE\n6030\n\n\nvehicular\nTRAFFIC VIOLATION|CALL FOR SERVICE\n2\n\n\nvehicular\nCALL FOR SERVICE|TRAFFIC VIOLATION\n2\n\n\nvehicular\nTRAFFIC VIOLATION|OTHER\n1\n\n\nvehicular\nSUSPECT VEHICLE|SUSPECT PERSON|SUSPECT PERSON\n1\n\n\nvehicular\nCRIMINAL VIOLATION|TRAFFIC VIOLATION\n1\n\n\nvehicular\nTRAFFIC VIOLATION|OTHER|OTHER\n1\n\n\n\n\n\nLastly, I looked at some common reasons behind the traffic stops in New Orleans over the total amount of time this data was collected. Interestingly, pedestrian stops are unanimously due to “suspect person” and nothing related to traffic while vehicular stops are almost certainly related to traffic misdemeanors.\n\ndbDisconnect(con_traffic, shutdown = TRUE)\n\nIn conclusion, I found that in New Orleans 2018, there was almost four times the amount of vehicular traffic stops that involved a black subject than white subject, these stops seem to be consistently increasing through 2010 to 2015, and pedestrian stops seem to be due to non-traffic reasons while vehicular stops are almost always related to traffic reasons for the amount of time this study has been carried out.\nReference.\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10."
  },
  {
    "objectID": "SQL.html#do-the-total-number-of-traffic-stops-differ-as-a-function-of-race",
    "href": "SQL.html#do-the-total-number-of-traffic-stops-differ-as-a-function-of-race",
    "title": "Stanford Open Policing Project",
    "section": "Do the total number of traffic stops differ as a function of race?",
    "text": "Do the total number of traffic stops differ as a function of race?\n\nSELECT \n  subject_race, \n  COUNT(*) AS n_stops\nFROM la_new_orleans_2020_04_01\nWHERE type = 'vehicular'\n  AND subject_race IN ('White', 'Black')\n  AND STR_TO_DATE(CONCAT(SUBSTR(date, 1, 4), '-01-01'), '%Y-%m-%d') = '2018-01-01'\nGROUP BY subject_race\nORDER BY n_stops DESC;\n\n\n2 records\n\n\nsubject_race\nn_stops\n\n\n\n\nblack\n16087\n\n\nwhite\n4389\n\n\n\n\n\nTo narrow down the focus and avoiding to deal with too much data, I decided to look at the total amount of vehicular traffic stops in New Orleans in 2018 separated by black or white subjects. Here I found that the amount of incidents where a black subjects is stopped on the road is almost four times the amount that of white subjects. Even though, according to online sources, African Americans makes up approximately 60% of the population in New Orleans in 2018, whereas Whites only 30%, it is still quite disproportional that Black subjects are stopped four times as much as white subjects.\n\nWhat about a state that are predominantly white?\n\nSELECT \n  subject_race, \n  COUNT(*) AS n_stops\nFROM vt_statewide_2020_04_01\nWHERE type = 'vehicular'\n  AND subject_race IN ('white', 'black')\n  AND STR_TO_DATE(CONCAT(SUBSTR(date, 1, 4), '-01-01'), '%Y-%m-%d') = '2015-01-01'\nGROUP BY subject_race\nORDER BY n_stops DESC;\n\n\n2 records\n\n\nsubject_race\nn_stops\n\n\n\n\nwhite\n42449\n\n\nblack\n1081\n\n\n\n\n\n\n\nNew Orleans VS Vermont\n# Load required packages\nlibrary(tidyverse)\nlibrary(patchwork)\n\n# Data setup\ndata &lt;- tibble(\n  area = c(rep(\"New Orleans 2018\", 2), rep(\"Vermont 2015\", 2)),\n  race = rep(c(\"Black\", \"White\"), 2),\n  traffic_stops = c(16087, 4389, 1081, 42449),\n  population_percent = c(59, 31, 1.2, 93)\n)\n\n# Separate datasets\nnola_data &lt;- data %&gt;% filter(area == \"New Orleans 2018\")\nvermont_data &lt;- data %&gt;% filter(area == \"Vermont 2015\")\n\n# Plots for New Orleans\nnola_stops_plot &lt;- ggplot(nola_data, aes(x = race, y = traffic_stops, fill = race)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Traffic Stops - New Orleans (2018)\", y = \"Number of Stops\", x = NULL) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\nnola_pop_plot &lt;- ggplot(nola_data, aes(x = race, y = population_percent, fill = race)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Population % - New Orleans (2018)\", y = \"Population %\", x = NULL) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Plots for Vermont\nvt_stops_plot &lt;- ggplot(vermont_data, aes(x = race, y = traffic_stops, fill = race)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Traffic Stops - Vermont (2015)\", y = \"Number of Stops\", x = NULL) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\nvt_pop_plot &lt;- ggplot(vermont_data, aes(x = race, y = population_percent, fill = race)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Population % - Vermont (2015)\", y = \"Population %\", x = NULL) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Combine plots using patchwork\n(nola_stops_plot | nola_pop_plot) / (vt_stops_plot | vt_pop_plot)\n\n\n\n\n\n\n\n\n\nI looked at Vermont for the state that is consisted of predominantly white population (almost 93% in 2015, according the Vermont Department of Health), and not surprisingly the number of traffic stops matched the general demographics. However, this is not to say that traffic stops are not, at all, affected by the race of the subject, even though this very crude analysis do show a simple fact which is that the general demographic of a region does matches the racial trends in traffic tops. Further correlation analysis would be required to draw statistically significant conclusions."
  },
  {
    "objectID": "SQL.html#reference",
    "href": "SQL.html#reference",
    "title": "Stanford Open Policing Project",
    "section": "Reference",
    "text": "Reference\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10."
  },
  {
    "objectID": "SQL.html#citation",
    "href": "SQL.html#citation",
    "title": "Stanford Open Policing Project",
    "section": "Citation",
    "text": "Citation\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10."
  },
  {
    "objectID": "SQL.html#do-the-total-amount-of-traffic-stops-differ-over-time",
    "href": "SQL.html#do-the-total-amount-of-traffic-stops-differ-over-time",
    "title": "Stanford Open Policing Project",
    "section": "Do the total amount of traffic stops differ over time?",
    "text": "Do the total amount of traffic stops differ over time?\n\nSELECT \n  SUBSTR(date, 1, 4) AS year,\n  COUNT(*) AS n_stops\nFROM la_new_orleans_2020_04_01\nWHERE type = 'vehicular'\n  AND STR_TO_DATE(CONCAT(vehicle_year, '-01-01'), '%Y-%m-%d') BETWEEN '2010-01-01' AND '2018-12-31'\nGROUP BY year\nORDER BY year;\n\n\n9 records\n\n\nyear\nn_stops\n\n\n\n\n2010\n550\n\n\n2011\n1983\n\n\n2012\n2407\n\n\n2013\n2311\n\n\n2014\n5764\n\n\n2015\n10238\n\n\n2016\n8931\n\n\n2017\n13515\n\n\n2018\n9184\n\n\n\n\n\nContinuing from the previous query, I decided to look at the amount of vehicular stops in New Orleans every year since 2010 to 2018. The first 6 years show a fairly consistent increase in number of stops, which I suspect that it has to do with increased ownership of cars and population growth.\n\nWhat are some common reasons for vehicular and pedestrian stops\n\n\nSELECT \n  type,\n  reason_for_stop,\n  COUNT(*) AS n_stops\nFROM la_new_orleans_2020_04_01\nWHERE type IN ('pedestrian', 'vehicular')\nGROUP BY type, reason_for_stop\nORDER BY type, n_stops DESC;\n\n\n9 records\n\n\n\n\n\n\n\ntype\nreason_for_stop\nn_stops\n\n\n\n\npedestrian\nSUSPECT PERSON\n63829\n\n\nvehicular\nTRAFFIC VIOLATION\n292318\n\n\nvehicular\nSUSPECT VEHICLE\n6030\n\n\nvehicular\nTRAFFIC VIOLATION|CALL FOR SERVICE\n2\n\n\nvehicular\nCALL FOR SERVICE|TRAFFIC VIOLATION\n2\n\n\nvehicular\nTRAFFIC VIOLATION|OTHER\n1\n\n\nvehicular\nSUSPECT VEHICLE|SUSPECT PERSON|SUSPECT PERSON\n1\n\n\nvehicular\nCRIMINAL VIOLATION|TRAFFIC VIOLATION\n1\n\n\nvehicular\nTRAFFIC VIOLATION|OTHER|OTHER\n1\n\n\n\n\n\nLastly, I looked at some common reasons behind the traffic stops in New Orleans over the total amount of time this data was collected. Interestingly, pedestrian stops are unanimously due to “suspect person” and nothing related to traffic while vehicular stops are almost certainly related to traffic misdemeanors.\n\ndbDisconnect(con_traffic, shutdown = TRUE)\n\nIn conclusion, I found that in New Orleans 2018, there was almost four times the amount of vehicular traffic stops that involved a black subject than white subject, these stops seem to be consistently increasing through 2010 to 2015, and pedestrian stops seem to be due to non-traffic reasons while vehicular stops are almost always related to traffic reasons for the amount of time this study has been carried out."
  },
  {
    "objectID": "SQL.html#do-the-total-number-of-traffic-stops-differ-by-race",
    "href": "SQL.html#do-the-total-number-of-traffic-stops-differ-by-race",
    "title": "Stanford Open Policing Project",
    "section": "Do the total number of traffic stops differ by race?",
    "text": "Do the total number of traffic stops differ by race?\n\nSELECT \n  subject_race, \n  COUNT(*) AS n_stops\nFROM la_new_orleans_2020_04_01\nWHERE type = 'vehicular'\n  AND subject_race IN ('White', 'Black')\n  AND STR_TO_DATE(CONCAT(SUBSTR(date, 1, 4), '-01-01'), '%Y-%m-%d') = '2018-01-01'\nGROUP BY subject_race\nORDER BY n_stops DESC;\n\n\n2 records\n\n\nsubject_race\nn_stops\n\n\n\n\nblack\n16087\n\n\nwhite\n4389\n\n\n\n\n\nTo limit the scope of the data, I focused on vehicular stops in New Orleans in 2018 involving Black or White individuals. The results show that Black individuals were stopped nearly four times more often than White individuals. While approximately 60% of New Orleans’ population in 2018 was Black (compared to 30% White), this disparity still suggests a potential racial imbalance in stop rates.\n\nWhat about a predominantly White state?\n\nSELECT \n  subject_race, \n  COUNT(*) AS n_stops\nFROM vt_statewide_2020_04_01\nWHERE type = 'vehicular'\n  AND subject_race IN ('white', 'black')\n  AND STR_TO_DATE(CONCAT(SUBSTR(date, 1, 4), '-01-01'), '%Y-%m-%d') = '2015-01-01'\nGROUP BY subject_race\nORDER BY n_stops DESC;\n\n\n2 records\n\n\nsubject_race\nn_stops\n\n\n\n\nwhite\n42449\n\n\nblack\n1081\n\n\n\n\n\n\n\nNew Orleans vs Vermont\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(patchwork)\n\n# Manually create a summary of stops and population demographics\ndata &lt;- tibble(\n  area = c(rep(\"New Orleans 2018\", 2), rep(\"Vermont 2015\", 2)),\n  race = rep(c(\"Black\", \"White\"), 2),\n  traffic_stops = c(16087, 4389, 1081, 42449),\n  population_percent = c(59, 31, 1.2, 93)\n)\n\n# Subsets for each area\nnola_data &lt;- data %&gt;% filter(area == \"New Orleans 2018\")\nvermont_data &lt;- data %&gt;% filter(area == \"Vermont 2015\")\n\n# Create bar plots for traffic stops and population %\nnola_stops_plot &lt;- ggplot(nola_data, aes(x = race, y = traffic_stops, fill = race)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Traffic Stops - New Orleans (2018)\",\n    y = \"Number of Stops\", x = NULL,\n    caption = \"Data Source: Stanford Open Policing Project\"\n  ) +\n  theme_minimal() + theme(legend.position = \"none\")\n\nnola_pop_plot &lt;- ggplot(nola_data, aes(x = race, y = population_percent, fill = race)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Population % - New Orleans (2018)\",\n    y = \"Population %\", x = NULL,\n    caption = \"Data Source: U.S. Census\"\n  ) +\n  theme_minimal() + theme(legend.position = \"none\")\n\nvt_stops_plot &lt;- ggplot(vermont_data, aes(x = race, y = traffic_stops, fill = race)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Traffic Stops - Vermont (2015)\",\n    y = \"Number of Stops\", x = NULL,\n    caption = \"Data Source: Stanford Open Policing Project\"\n  ) +\n  theme_minimal() + theme(legend.position = \"none\")\n\nvt_pop_plot &lt;- ggplot(vermont_data, aes(x = race, y = population_percent, fill = race)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Population % - Vermont (2015)\",\n    y = \"Population %\", x = NULL,\n    caption = \"Data Source: Vermont Department of Health\"\n  ) +\n  theme_minimal() + theme(legend.position = \"none\")\n\n# Combine plots\n(nola_stops_plot | nola_pop_plot) / (vt_stops_plot | vt_pop_plot)\n\n\n\n\n\n\n\n\n\nFigure: Traffic stops and population demographics in New Orleans and Vermont.\n\nAlt text: Four bar plots comparing racial demographics and traffic stops in New Orleans (2018) and Vermont (2015).\nIn Vermont, where over 90% of the population is White (according to the Vermont Department of Health), the number of traffic stops reflects the demographic majority. Although a crude analysis, it suggests demographic patterns play a role in stop distribution. More rigorous statistical tests would be needed to investigate systemic bias."
  },
  {
    "objectID": "SQL.html#do-traffic-stops-vary-over-time",
    "href": "SQL.html#do-traffic-stops-vary-over-time",
    "title": "Stanford Open Policing Project",
    "section": "Do traffic stops vary over time?",
    "text": "Do traffic stops vary over time?\n\nSELECT \n  SUBSTR(date, 1, 4) AS year,\n  COUNT(*) AS n_stops\nFROM la_new_orleans_2020_04_01\nWHERE type = 'vehicular'\n  AND STR_TO_DATE(CONCAT(vehicle_year, '-01-01'), '%Y-%m-%d') BETWEEN '2010-01-01' AND '2018-12-31'\nGROUP BY year\nORDER BY year;\n\n\n9 records\n\n\nyear\nn_stops\n\n\n\n\n2010\n550\n\n\n2011\n1983\n\n\n2012\n2407\n\n\n2013\n2311\n\n\n2014\n5764\n\n\n2015\n10238\n\n\n2016\n8931\n\n\n2017\n13515\n\n\n2018\n9184\n\n\n\n\n\nThis query returns annual traffic stop counts in New Orleans from 2010 to 2018. The data shows an upward trend in stops from 2010 to 2015. This may reflect increased vehicle ownership, population growth, or improved data collection systems."
  },
  {
    "objectID": "SQL.html#what-are-common-reasons-for-vehicular-and-pedestrian-stops",
    "href": "SQL.html#what-are-common-reasons-for-vehicular-and-pedestrian-stops",
    "title": "Stanford Open Policing Project",
    "section": "What are common reasons for vehicular and pedestrian stops?",
    "text": "What are common reasons for vehicular and pedestrian stops?\n\nSELECT \n  type,\n  reason_for_stop,\n  COUNT(*) AS n_stops\nFROM la_new_orleans_2020_04_01\nWHERE type IN ('pedestrian', 'vehicular')\nGROUP BY type, reason_for_stop\nORDER BY type, n_stops DESC;\n\n\n9 records\n\n\n\n\n\n\n\ntype\nreason_for_stop\nn_stops\n\n\n\n\npedestrian\nSUSPECT PERSON\n63829\n\n\nvehicular\nTRAFFIC VIOLATION\n292318\n\n\nvehicular\nSUSPECT VEHICLE\n6030\n\n\nvehicular\nTRAFFIC VIOLATION|CALL FOR SERVICE\n2\n\n\nvehicular\nCALL FOR SERVICE|TRAFFIC VIOLATION\n2\n\n\nvehicular\nTRAFFIC VIOLATION|OTHER\n1\n\n\nvehicular\nSUSPECT VEHICLE|SUSPECT PERSON|SUSPECT PERSON\n1\n\n\nvehicular\nCRIMINAL VIOLATION|TRAFFIC VIOLATION\n1\n\n\nvehicular\nTRAFFIC VIOLATION|OTHER|OTHER\n1\n\n\n\n\n\nIn New Orleans, “traffic violation” is the most cited reason for vehicular stops, while all pedestrian stops were classified under “suspect person.” Let’s see how this compares with data from Grand Forks, North Dakota.\n\nSELECT \n  reason_for_stop,\n  COUNT(*) AS n_stops\nFROM nd_grand_forks_2020_04_01\nWHERE type = 'pedestrian'\nGROUP BY reason_for_stop\nORDER BY n_stops DESC;\n\n\nDisplaying records 1 - 10\n\n\nreason_for_stop\nn_stops\n\n\n\n\nDISORDERLY CONDUCT\n2063\n\n\nSIMPLE ASSAULT\n1066\n\n\nPUBLIC CONSUMPTION ON STREETS PROHIBITED\n441\n\n\nDRUG PARAPHERNALIA-POSSESSION\n323\n\n\nAGGRAVATED ASSAULT\n251\n\n\nCRIMINAL TRESPASS\n210\n\n\nCONTROLLED SUBSTANCE\n199\n\n\nPOSSESSION MARIJUANA\n173\n\n\nTRESPASS ON PRIVATE PROPERTY\n159\n\n\nOPEN CONTAINER PROHIBITED\n155\n\n\n\n\n\n\nSELECT \n  reason_for_stop,\n  COUNT(*) AS n_stops\nFROM nd_grand_forks_2020_04_01\nWHERE type = 'vehicular'\nGROUP BY reason_for_stop\nORDER BY n_stops DESC;\n\n\nDisplaying records 1 - 10\n\n\nreason_for_stop\nn_stops\n\n\n\n\nSPEED LIMITS\n13979\n\n\nROW AT INTERSECTION-FAIL TO YIELD\n5376\n\n\nFAIL DISPLAY LIC PLATE/TAB\n4371\n\n\nTRAFFIC SIGNAL DISREGARD\n2142\n\n\nFOLLOW TOO CLOSE\n1584\n\n\nDUI\n1363\n\n\nFAIL TO HAVE VEH UNDER CONTROL\n1084\n\n\nSEAT BELTS TO BE WORN\n1066\n\n\nSEAT BELTS TO BE WORN|SPEED LIMITS\n711\n\n\nUNQUALIFIED OPERATOR\n634\n\n\n\n\n\n\n\nReasons for Stops in Grand Forks\nlibrary(tidyverse)\n\n# Create top reason data for Grand Forks\npedestrian_df &lt;- tibble(\n  reason = c(\n    \"DISORDERLY CONDUCT\",\n    \"SIMPLE ASSAULT\",\n    \"PUBLIC CONSUMPTION ON STREETS PROHIBITED\",\n    \"DRUG PARAPHERNALIA-POSSESSION\",\n    \"AGGRAVATED ASSAULT\"\n  ),\n  n_stops = c(2063, 1066, 441, 323, 251)\n)\n\nvehicular_df &lt;- tibble(\n  reason = c(\n    \"SPEED LIMITS\",\n    \"ROW AT INTERSECTION-FAIL TO YIELD\",\n    \"FAIL DISPLAY LIC PLATE/TAB\",\n    \"TRAFFIC SIGNAL DISREGARD\",\n    \"FOLLOW TOO CLOSE\"\n  ),\n  n_stops = c(13979, 5376, 4371, 2142, 1584)\n)\n\n# Plot pedestrian reasons\npedestrian_plot &lt;- ggplot(pedestrian_df, aes(x = reorder(reason, n_stops), y = n_stops)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Top 5 Reasons for Pedestrian Stops (Grand Forks)\",\n    x = \"Reason for Stop\",\n    y = \"Number of Stops\",\n    caption = \"Data Source: Stanford Open Policing Project\"\n  ) +\n  theme_minimal()\n\n# Plot vehicular reasons\nvehicular_plot &lt;- ggplot(vehicular_df, aes(x = reorder(reason, n_stops), y = n_stops)) +\n  geom_col(fill = \"darkred\") +\n  coord_flip() +\n  labs(\n    title = \"Top 5 Reasons for Vehicular Stops (Grand Forks)\",\n    x = \"Reason for Stop\",\n    y = \"Number of Stops\",\n    caption = \"Data Source: Stanford Open Policing Project\"\n  ) +\n  theme_minimal()\n\npedestrian_plot\n\n\n\n\n\n\n\n\n\nReasons for Stops in Grand Forks\nvehicular_plot\n\n\n\n\n\n\n\n\n\nFigure: Top 5 reasons for pedestrian and vehicular stops in Grand Forks.\n\nAlt text: Two bar charts showing leading causes of police stops for pedestrians and vehicles in Grand Forks, North Dakota.\nIn contrast to New Orleans, Grand Forks lists more specific categories like “Disorderly Conduct” for pedestrians and “Speed Limits” for vehicles. This likely reflects differences in data categorization across jurisdictions.\n\ndbDisconnect(con_traffic, shutdown = TRUE)"
  },
  {
    "objectID": "SQL.html#conclusion",
    "href": "SQL.html#conclusion",
    "title": "Stanford Open Policing Project",
    "section": "Conclusion",
    "text": "Conclusion\nThis exploratory analysis revealed that traffic stop counts often reflect local demographics but may still show racial disparities, especially in diverse cities like New Orleans. Additionally, the volume of stops has generally increased over time in certain regions, and the reasons for stops vary notably by region and stop type (vehicular vs pedestrian).\n\nWho collected this data and why?\nThe Stanford Open Policing Project (SOPP) was launched by researchers at Stanford University, including Emma Pierson, Camelia Simoiu, and others. The project aimed to analyze racial disparities in policing by compiling and standardizing traffic stop data from across the United States. Their goal was to provide an empirical foundation for understanding how race may influence police behavior and to inform discussions on law enforcement practices and policy reform."
  }
]