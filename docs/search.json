[
  {
    "objectID": "Childcare Costs.html",
    "href": "Childcare Costs.html",
    "title": "Childcare Costs",
    "section": "",
    "text": "This dataset comes from the National Database of Childcare Prices (NDCP), which is (one of) the most comprehensive federal source of childcare at the county level. Data provided by Thomas Mock.\n\nlibrary(tidyverse)\n\nchildcare_costs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2023/2023-05-09/childcare_costs.csv')\n\nchildcare_costs |&gt;\nggplot( aes( x = mcsa, y = pr_f ) ) +\n  geom_point(color = \"blue\", alpha = 0.1) +\n  labs( x = \"median price charged for center-based care who are school age\",\n        y = \"poverty rate for familie\" )\n\n\n\n\n\n\n\n\nData taken from TidyTuesday GitHub Repository: https://github.com/rfordatascience/tidytuesday/tree/main/data\nData originally sourced from National Database of Childcare Prices:\nhttps://www.dol.gov/agencies/wb/topics/featured-childcare"
  },
  {
    "objectID": "The Scent of Data.html",
    "href": "The Scent of Data.html",
    "title": "The Scent of Data",
    "section": "",
    "text": "This dataset contains information about perfumes webscraped from Parfumo (community of perfume enthusiasts) by Olga G..\n\nlibrary(dplyr)\nlibrary(tidyverse)\n\nparfumo_data_clean &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-12-10/parfumo_data_clean.csv')\n\nbrand_vs_rating &lt;- parfumo_data_clean |&gt;\n  group_by(Brand) |&gt;\n  summarize( Average_Rating_Value = mean(Rating_Value, na.rm = TRUE) ) |&gt;\n  arrange( desc( Average_Rating_Value ) ) \n\ntop10_brand_vs_rating &lt;- head( brand_vs_rating, 10)\n  \nggplot( top10_brand_vs_rating, aes( x = Brand, y = Average_Rating_Value, fill = Brand ) ) +\n  geom_bar( stat = \"identity\") +\n  theme(axis.text.x = element_blank())\n\n\n\n\n\n\n\n\nData taken from TidyTuesday GitHub Repository: https://github.com/rfordatascience/tidytuesday/tree/main/data\nData originally sourced from Perfumo:\nhttps://www.parfumo.com/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zhesheng(Jason) Xu",
    "section": "",
    "text": "Hi there! I’m Jason, my hometown is Shanghai, China! I am currently a student at Pomona College, CA, where I am studying neuroscience and data science. I am also a part of a club called Health Bridges where I get to volunteer at Pomona Valley Hospital as a non-medical translator! In my free time, I like to play the guitar (Chinese folk songs are my favorite!). Look around my website to learn more!\nI used Quarto to build this website, the link to the repository is here: https://github.com/ASTROXXXXX/ASTROXXXXX.github.io"
  },
  {
    "objectID": "Strings.html",
    "href": "Strings.html",
    "title": "Macbeth",
    "section": "",
    "text": "This dataset contains the entire script of the famous Shakesqeare play: Macbeth. As a side note, I was actually really into acting back in middle school, and I actually played a character (who’s name I forgot - not a big character) in Macbeth during one of the school shows. I’m excited, let’s go ahead and play with the data :)\n\nlibrary(dplyr)\nlibrary(tidyverse)\n\nmacbeth &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-09-17/macbeth.csv')\n\nneg_lines &lt;- macbeth |&gt;\n  mutate( blood = str_detect( dialogue, \"(?i)blood|(?i)murder[er]|(?i)death|(?i)kill\" ) ) |&gt;\n  filter( blood ) |&gt;\n  group_by( act ) |&gt;\n  summarize( average_negative_lines = n() ) \n\naverage_words &lt;- macbeth |&gt;\n  mutate( string_length = str_length( dialogue ) ) |&gt;\n  group_by( act ) |&gt;\n  summarize( average_words = mean( string_length ) ) \n\nnegative_words &lt;- left_join( neg_lines, average_words )\n\nggplot( negative_words, aes(x = average_words, y = average_negative_lines, label = act)) +\n  geom_point(color = \"blue\", size = 3) +  \n  geom_text(vjust = -1, size = 4) +       \n  labs(title = \"Average Words vs. Negative Lines in each act of Macbeth\",\n       x = \"Average Words per Act\",\n       y = \"Number of Lines with Negative Words\" ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHere, I counted the number of lines in each act of Macbeth that contained one or more “negative words”, which I defined to be “blood”, “murder(er)”, “death” and “kill”. Then, it is paired up with the average number of words in each act (see figure above). I was interested in knowing if a particular act had more lines containing “negative words” than perhaps the characters spoke more on average (or less, there is some relationship is the point). However, just by eyeballing the above plot, it seems like there is no clear relationship between the two. (noticeably, however, Act III is a pretty extreme outlier, if we ignore it the plot actually shows a pretty down-wards trending relationship).\n\nmacbeth |&gt;\n  filter( character == \"Macbeth\" ) |&gt;\n  mutate( love_strings = str_extract( dialogue, \"(?&lt;=(?i)love ).+\") ) |&gt;\n  filter( !is.na(love_strings) ) |&gt;\n  select( character:love_strings, -line_number )\n\n# A tibble: 4 × 3\n  character dialogue                                             love_strings   \n  &lt;chr&gt;     &lt;chr&gt;                                                &lt;chr&gt;          \n1 Macbeth   Safe toward your love and honour.                    and honour.    \n2 Macbeth   Courage to make 's love kno wn?                      kno wn?        \n3 Macbeth   Grapples you to the heart and love of us,            of us,         \n4 Macbeth   To those that know me. Come, love and health to all; and health to …\n\n\nAfter looking at the negative, I decided to turn to the opposite — whether if Macbeth, the main character of the play, has explicity expressed “love” in the play, and to whom. So I used the lookaround function to spot instances of Macbeth saying the word “love” and the “love_strings” is whatever comes after “love”. Sadly, these instances only occured 4 times in the whole play and they seem to occur only in more generic settings (like toasting) — poor lady Macbeth :(\nData taken from TidyTuesday GitHub Repository: https://github.com/rfordatascience/tidytuesday/tree/main/data\nData originally sourced from:\nhttps://shakespeare.mit.edu/"
  },
  {
    "objectID": "Simulation Analysis.html",
    "href": "Simulation Analysis.html",
    "title": "Simulation!",
    "section": "",
    "text": "Assuming that there are four course registration periods over four years of a student’s time at Pomona College (there are actually 8), and course registration happens once per year. Suppose that (drawing from personal experience) the college’s algorithm is designed so that it categorizes registration times into:\n1. Early: 8 ~ 10am.\n2. Moderately Early: 10 ~ noon.\n3. Moderately Late: noon ~ 2pm.\n4. Late: 2 ~ 4pm.\nand suppose that during course registration the algorithm first randomly determines a category and then randomly assigns a time (by each minute) within that category to each student and also makes sure that each category is assigned to each student only once over their four years at the college (i.e four course registrations).\nSuppose that the best thing to happen is either A. a freshman gets a time before 9am (so that he/she can beat the other freshmen in the same category) B. a sophomore gets a time before 11am. I want to use simulation to find the probability of A., B. and for the luckiest ones, A. and B. both happen.\n\nlibrary(tidyverse)\nset.seed(47)\n\ncorsreg_func &lt;- function(banana){\n  # shuffle categories\n  cats = sample(c(1:4), 4, replace = FALSE)\n  # a random time in 2 hours\n  times = sample(c(1:120), 4, replace = TRUE)\n\n  data.frame(\n  lucky_freshman = (cats[1] == 1 && times[1] &lt;= 59),\n  lucky_sophomore = (cats[2] == 1 | cats[2] == 2 && times[2] &lt;= 59),\n  luckiest_one = ((cats[1] == 1 && times[1] &lt;= 59) && (cats[2] == 1 | cats[2] == 2 && times[2] &lt;= 59)))\n  }\n\nn &lt;- 1761\ncorsreg_data &lt;- map(1:n, corsreg_func) |&gt;\n  list_rbind()\n\nsummary_data &lt;- corsreg_data |&gt;\n  summarise(\n    lucky_freshman_prop = mean(lucky_freshman),\n    lucky_sophomore_prop = mean(lucky_sophomore),\n    luckiest_one_prop = mean(luckiest_one)\n  ) |&gt;\n  pivot_longer(cols = everything(), names_to = \"category\", values_to = \"proportion\")\n\n\nsummary_data$category &lt;- factor(summary_data$category, \n                                 levels = c(\"lucky_freshman_prop\", \"lucky_sophomore_prop\", \"luckiest_one_prop\"))\n\n\nggplot(summary_data, aes(x = category, y = proportion, fill = category)) +\n  geom_bar(stat = \"identity\", width = 0.6) +\n  labs(\n    title = \"Course Registration Simulation at Pomona College\",\n    x = \"I simulated 1761 times to approximate the number of students on campus\",\n    y = \"Proportion\"\n  ) +\n  theme_minimal() +\n  geom_text(aes(label = round(proportion, 3)), vjust = -0.5) +  \n  ylim(0, 1) +\n  theme(axis.text.x = element_blank())\n\n\n\n\n\n\n\n\nSeems like there is around 11% probability for a student to be lucky in their course registrations during their freshman year, and a much greater probability (around 36%) for him/her to be lucky in their sophomore year. But getting extremely lucky, where he/she get an early time for both years is extremely rare (around 2%). Despite the fact that I believe getting a time before 9am is pretty important for a freshman to get into a class they wanted, knowing that there is almost 40% students who will get an ideal time in their sophomore year is pretty relieving. Some lucky sophomores may also be lucky in their freshman year, but that rate is low (around 2%). I personally think that the algorithm, as presented here, seems to be decent, because we are getting ideal results for almost 40% of the students in their sophomore year (which is pretty high). But things could also be said about the pretty low rate (11%) to be lucky freshman. But it is also important to note that we have other mechanisms to distribute courses to students who actually need them (or need them more), such as the PERM system or considerations on individual basis."
  },
  {
    "objectID": "Ethics.html",
    "href": "Ethics.html",
    "title": "Ethics in Data Science",
    "section": "",
    "text": "TEST\nScenario.\nThe COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) algorithm is developed by a for-profit company called Northpointe. It is used in courtrooms across the US to help inform sentencing decisions, set bail/bond amounts, and other decision-makings in the criminal justice system (Angwin et al, 2016). The algorithm’s key feature is to produce “risk ratings” for defendants based on their criminal history and their response to a questionnaire of more than 100 questions (Angwin et al, 2016). The ethical consideration is that the algorithm seems to be racial and gender sensitive. It was found that it is “particularly likely to falsely flag black defendants as future criminals, wrongly labeling them this way at almost twice the rate as white defendants” (Angwin et al, 2016), and “women were rated two classes worse then men”. (Drosser et al, 2020).\nIt is easy to conclude that the algorithm is biased/or even racist/sexist against defendants, but it may be a bit deeper than that. For one thing, the algorithm seems “accurate” in predicting recidivism rates across race. In other words, there is a similar proportion of white defendants who will re-offend a crime compared to black defendants of the same risk rating (Corbett-Davies et al, 2018), and that seems to be what the risk ratings were initially designed to embody. It has been proposed that, maybe it is impossible for an algorithm to achieve both: 1. well calibrated between groups, where the risk ratings mean the same thing between groups. 2. not falsely flag certain groups more frequently than others (this is called false positives). Maybe it is achievable in a perfect society when everybody in every group receive the same amount of resources, education, etc. But in this world, where inequality and discrimination is not uncommon, especially from the past, racial groups unfortunately may be correlated with the likelihood to commit (or recommit) a crime (this is known as base rate differences). This correlation may be the reason why it may be impossible to do the two things aforementioned (This argument is mainly derived from Corbett-Davies et al, 2018).\nQuestions.\nWhat was the data collection process? Were the observations collected ethically? Are there missing observations?\nAn issue with the data collection process could be that data is acquired without the subject’s consent or even without their knowledge. The application of this to the example above could be a bit tricky, because the subjects are suspects of certain crimes. In the algorithm, data are drawn from the defendant’s criminal records (probably automatically) and the defendant’s are given some questions to answer. The fact that a private company can have access to someone’s criminal records may or may not be an issue, but some questions do seem a bit problematic. For example a question would ask if the defendant agree or disagree with statements like “A hungry person has a right to steal” and “If people make me angry or lose my temper, I can be dangerous” (Angwin et al, 2016).\nIs the data being used in unintended ways to the original study?\nYes, the algorithm is invented by Tim Brennan, former professor of statistics at the University of Colorado. He later sold the algorithm to a for-profit company that currently runs the algorithm. Brennan claimed that his original focus was for the algorithm to “reduce crime rather than punish”, and he “does not like the idea that COMPAS being the sole evidence that a decision would be based upon” (Angwin et al, 2016). But whether he likes it or not, as the algorithm got adopted by many courts and entities in the criminal justice system across the country, and its use has definitely departed from what is original intended.\nShould race be used as a variable? Is it a proxy for something else? What about gender?\nI think whether race is explicitly used as a variable or not, many algorithms, COMPAS included, still effectively “uses” race via other proxy attributes of it (same goes for gender). An interesting thing is that, according to Northpointe, race, gender or any other socially protected categories are actually not explicitly asked in their questions or entered in the algorithm (Angwin et al, 2016). But there is many reason to believe that the algorithm, alternatively, uses other “proxies” for race and gender. For one, the algorithm clearly seems to be predicting female defendants to be “riskier” than male defendants, by 2 risk ratings, according to a study (Drosser et al, 2020).\nDid it create reproducible and extensible work?\nIt didn’t. The questions and calculations are mostly not disclosed to the public because of propriety, which made it almost impossible to reproduce the calculations and obtain the same risk ratings (Angwin et al, 2016). More importantly, defendants “rarely have the opportunity to challenge their assessments” because the calculations and how the ratings are calculated is not revealed. This may be unfair to the defendants, and could potentially be detrimental if the risk rating got it wrong.\nCitations.\n\nAngwin, J., Larson, J., Mattu, S., & Kirchner, L. (2016, May 23). Machine bias. ProPublica. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.\nCorbett-Davies, S., & Goel, S. (2018, October 17). A computer program used for bail and sentencing decisions was labeled biased against blacks. It’s actually not that clear. The Washington Post. https://www.washingtonpost.com/news/monkey-cage/wp/2018/10/17/a-computer-program-used-for-bail-and-sentencing-decisions-was-labeled-biased-against-blacks-its-actually-not-that-clear/.\nDrosser, C. (2020, August 5). In order not to discriminate, we might have to discriminate. The New York Times. https://www.nytimes.com/2020/08/05/magazine/algorithm-discrimination.html."
  },
  {
    "objectID": "SQL.html",
    "href": "SQL.html",
    "title": "Stanford Open Policing Project",
    "section": "",
    "text": "sadfsd"
  }
]