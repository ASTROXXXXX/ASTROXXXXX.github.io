---
title: "Ethics in Data Science"
format: html
---

## Scenario

The COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) algorithm was developed by a for-profit company called Northpointe. It is used in courtrooms across the U.S. to help inform sentencing decisions, set bail/bond amounts, and assist with other decision-making in the criminal justice system (Angwin et al., 2016). The algorithm's key feature is to produce "risk ratings" for defendants based on their criminal history and their responses to a questionnaire of more than 100 questions (Angwin et al., 2016). A major ethical concern is that the algorithm appears to be racially and gender sensitive. It was found to be “particularly likely to falsely flag Black defendants as future criminals, wrongly labeling them this way at almost twice the rate of white defendants” (Angwin et al., 2016), and that “women were rated two classes worse than men” (Drosser et al., 2020).

It is easy to conclude that the algorithm is biased—or even racist or sexist—against defendants, but the issue may be deeper than that. For one thing, the algorithm seems accurate in predicting recidivism rates across races. In other words, there is a similar proportion of white defendants who will re-offend compared to Black defendants with the same risk rating (Corbett-Davies et al., 2018), and that appears to be what the risk ratings were initially designed to reflect. It has been proposed that maybe it is impossible for an algorithm to achieve both:

-   Well-calibrated risk between groups (where risk ratings mean the same thing across groups), and

-   Equal false positive rates (not falsely flagging certain groups more than others).

Maybe this is achievable in a perfect society where everyone receives equal resources, education, etc. But in our current world—where inequality and systemic discrimination are common, especially due to historical factors—racial groups may unfortunately correlate with the likelihood to commit (or recommit) a crime. This is known as base rate differences. That correlation may be the reason why it’s impossible to achieve both fairness criteria at once. (This argument is mainly derived from Corbett-Davies et al., 2018.)

## What was the data collection process? Were the observations collected ethically? Are there missing observations?

An issue with the data collection process could be that data is acquired without the subject's consent—or even their knowledge. Applying this to the example above is a bit tricky, because the subjects are suspects in criminal cases. In the algorithm, data are drawn from defendants' criminal records (probably automatically), and the defendants are given some questions to answer. The fact that a private company can access someone's criminal records may or may not be an issue, but some of the questions do seem problematic. For example, one question asks whether the defendant agrees or disagrees with statements like:

> “A hungry person has a right to steal,” and “If people make me angry or lose my temper, I can be dangerous.” (Angwin et al., 2016)

## Is the data being used in unintended ways to the original study?

Yes. The algorithm was invented by Tim Brennan, a former professor of statistics at the University of Colorado. He later sold it to the for-profit company that currently runs it. Brennan claimed that his original focus was for the algorithm to “reduce crime rather than punish,” and he “does not like the idea that COMPAS \[is\] the sole evidence that a decision would be based upon” (Angwin et al., 2016). But whether he likes it or not, the algorithm has since been adopted by many courts and agencies in the criminal justice system across the country—and its use has definitely departed from its original intent.

## Should race be used as a variable? Is it a proxy for something else? What about gender?

I think whether race is explicitly used as a variable or not, many algorithms—COMPAS included—still effectively *use* race through proxy attributes (the same goes for gender). Interestingly, according to Northpointe, race, gender, or any other protected social categories are not explicitly asked about or entered into the algorithm (Angwin et al., 2016). However, there are many reasons to believe the algorithm uses other **proxies** for race and gender. For instance, the algorithm appears to rate female defendants as "riskier" than male defendants by **two full risk levels**, according to one study (Drosser et al., 2020).

## Did it create reproducible and extensible work?

It did not. The questions and calculations are mostly not disclosed to the public due to proprietary concerns, making it nearly impossible to reproduce the calculations or obtain the same risk ratings (Angwin et al., 2016). More importantly, defendants “rarely have the opportunity to challenge their assessments” because the calculation methods and logic behind the ratings are not revealed. This may be unfair to defendants and could be particularly harmful if the algorithm produces an inaccurate rating.

## Citations

1.  Angwin, J., Larson, J., Mattu, S., & Kirchner, L. (2016, May 23). Machine bias. ProPublica. <https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing>

2.  Corbett-Davies, S., & Goel, S. (2018, October 17). A computer program used for bail and sentencing decisions was labeled biased against blacks. It's actually not that clear. The Washington Post. <https://www.washingtonpost.com/news/monkey-cage/wp/2018/10/17/a-computer-program-used-for-bail-and-sentencing-decisions-was-labeled-biased-against-blacks-its-actually-not-that-clear/>

3.  Drosser, C. (2020, August 5). In order not to discriminate, we might have to discriminate. The New York Times. <https://www.nytimes.com/2020/08/05/magazine/algorithm-discrimination.html>
